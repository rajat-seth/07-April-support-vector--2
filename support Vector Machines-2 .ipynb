{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4262b905-2f30-415a-96e8-0256aa0dedb1",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ac812-8765-4721-aff5-cb2471a09751",
   "metadata": {},
   "source": [
    "The relationship between polynomial functions and kernel functions in machine learning algorithms is an essential concept, particularly in Support Vector Machines (SVM) and kernel methods. Polynomial functions serve as a type of kernel function used to transform the input data into a higher-dimensional space, enabling the SVM to find nonlinear decision boundaries.\n",
    "\n",
    "- Polynomial Functions and Kernel Functions Relationship:\n",
    "    - In machine learning, particularly in algorithms like Support Vector Machines (SVM), polynomial functions and kernel functions are intertwined to handle complex data patterns.\n",
    "- Addressing Nonlinear Data Patterns:\n",
    "    - Polynomial functions come into play when data points cannot be separated by a straight line or a flat plane in the original feature space.\n",
    "    - They help transform the data into a higher-dimensional space, where finding nonlinear boundaries between different classes becomes possible.\n",
    "- Enhancing Feature Space:\n",
    "    - Visualize it as moving our data to a different space, much like shifting to a different perspective to uncover hidden patterns.\n",
    "- Capturing Nonlinear Relationships:\n",
    "    - The goal is to uncover relationships and boundaries that were not apparent in the original data representation.\n",
    "- Mathematical Transformation:\n",
    "    - A polynomial kernel function mathematically represents this transformation.\n",
    "    - It computes the inner product of the transformed data points, without explicitly calculating the transformation itself.\n",
    "- Degree of the Polynomial:\n",
    "    - The degree of the polynomial determines the complexity of the transformation.\n",
    "    - Higher degrees can capture more intricate patterns but might also lead to overfitting.\n",
    "- Discovering Linear Boundaries in Higher Dimensions:\n",
    "    - In this transformed space, SVM can potentially find linear decision boundaries that correspond to nonlinear boundaries in the original space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f5749-064d-49b7-9cc4-62c476421073",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162ea37d-d757-4418-8e6d-50678fe71538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2987278f-591f-4508-96d8-b95fb84cc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (example: Iris dataset)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6289d368-7b61-45df-a1c0-2e55678cba43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an SVM model with polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3)  # You can adjust the degree parameter as needed\n",
    "\n",
    "# Train the SVM model\n",
    "svm_poly.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675e885-ef9d-44de-8814-f0e2fdb8d43c",
   "metadata": {},
   "source": [
    "That's it! we've successfully implemented an SVM with a polynomial kernel using Scikit-learn in Python. This approach allows you to handle nonlinear data patterns effectively and achieve accurate classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeaec20-aa3f-4650-b5d2-7c2748b989ea",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7553b71-cc91-48c7-8bc3-07fc200cd377",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), epsilon (ε) represents the margin of tolerance around the regression line. It defines a boundary within which errors are considered acceptable and do not contribute to the loss function. The relationship between the value of epsilon and the number of support vectors in SVR is as follows:\n",
    "\n",
    "- Smaller Epsilon (Tight Margin):\n",
    "\n",
    "    - When epsilon is set to a smaller value (tight margin), the SVR model becomes more sensitive to errors. It allows only data points that are very close to the regression line to be considered within the margin of tolerance.\n",
    "    - In this case, the SVR tries to fit the data as closely as possible, and as a result, it might lead to a larger number of support vectors. Support vectors are data points that lie either on the margin or   violate the margin due to their error terms.\n",
    "- Tight margins can result in overfitting, where the model may capture noise or outliers present in the data.\n",
    "- Larger Epsilon (Wider Margin):\n",
    "\n",
    "    - Increasing the value of epsilon (wider margin) makes the SVR model more tolerant to errors. It allows data points to fall within a wider margin around the regression line while still being considered acceptable.\n",
    "    - A wider margin results in fewer support vectors since data points within this broader margin are not penalized, even if they deviate from the regression line.\n",
    "    - Wider margins help in creating a simpler model that generalizes better to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528adc2-c67d-4096-bc48-ab4ddf835ee1",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c5488-b70d-41b1-8abf-71c911f0b7bf",
   "metadata": {},
   "source": [
    "Certainly, let's break down the effects of various parameters on the performance of Support Vector Regression (SVR), and how each parameter works, along with examples of when you might adjust their values.\n",
    "\n",
    "1. Kernel Function:\n",
    "The choice of the kernel function determines how SVR models nonlinear relationships. Common kernel functions include Linear, Polynomial, Radial Basis Function (RBF), and Sigmoid. The kernel transforms the input features into a higher-dimensional space to capture complex patterns.\n",
    "\n",
    "    - Effect: Different kernels capture different types of relationships. RBF kernel is versatile for various data distributions, while Polynomial kernel might capture specific polynomial relationships.\n",
    "    - Example: For highly complex data with intricate nonlinear patterns, RBF kernel might perform better. For data showing polynomial behavior, such as physics experiments, a Polynomial kernel could be more appropriate.\n",
    "2. C Parameter (Regularization):\n",
    "The C parameter controls the trade-off between fitting the training data and allowing deviations. Smaller C values lead to a wider margin and more tolerant model, while larger values result in a narrower margin, penalizing deviations more.\n",
    "\n",
    "    - Effect: Smaller C promotes a smoother, more generalized model, less prone to overfitting. Larger C fits the training data more closely but might lead to overfitting.\n",
    "    - Example: For financial data where outliers may be significant, a smaller C can help prevent overreacting to outliers. In applications where every point is crucial, a larger C may be suitable.\n",
    "3. Epsilon Parameter (Tolerance):\n",
    "Epsilon (ε) defines the width of the margin around the regression line within which errors are tolerated. Smaller epsilon focuses on precise fitting, while larger epsilon allows a wider range of errors.\n",
    "\n",
    "    - Effect: Smaller epsilon enforces tighter fitting, sensitive to individual data points. Larger epsilon allows more flexibility and handles noisy data.\n",
    "    - Example: In situations where data is inherently noisy, such as sensor measurements, a larger epsilon can prevent the model from being overly influenced by minor fluctuations.\n",
    "4. Gamma Parameter (RBF Kernel Specific):\n",
    "    - The gamma parameter defines the reach of a single training example's influence. Smaller gamma values make the influence broader, and larger values make it more localized.\n",
    "\n",
    "    - Effect: Smaller gamma results in smoother, less complex models, suited for simpler patterns. Larger gamma values create more complex, localized models.\n",
    "    - Example: In image classification, a larger gamma might be used to capture intricate local features. For a general trend prediction, a smaller gamma might lead to more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c1a45-347d-43bb-9e7a-ffdfca564c7a",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "-  Import the necessary libraries and load the dataseg\n",
    "-  Split the dataset into training and testing setZ\n",
    "-  Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "-  Create an instance of the SVC classifier and train it on the training datW\n",
    "-  hse the trained classifier to predict the labels of the testing datW\n",
    "-  Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "    precision, recall, F1-scoreK\n",
    "-  Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "   improve its performanc_\n",
    "-  Train the tuned classifier on the entire dataseg\n",
    "-  Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f096b05d-e926-450a-8b14-b2f914fac61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b66eaaf-9f4e-4599-904b-1507736a885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (example: Iris dataset)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7cb157-7dba-46d1-b5b5-c86b4dd5b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72948f67-8ddc-4e3f-8707-93d075491e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6870802-d010-44fb-83ad-9714981a3d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4173f1ea-cae4-47cc-afda-2b2e107a473d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
